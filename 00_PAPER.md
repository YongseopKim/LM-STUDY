# Paper

## NNLM
- A Neural Probabilistic Language Model
- Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, Christian Jauvin
- 2003
- https://dl.acm.org/doi/10.5555/944919.944966

## RNNLM

### RNN

### LSRM

### GRU
- Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
- Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio
- 2014
- https://arxiv.org/abs/1412.3555

## Word2vec

### word2vec
- Efficient Estimation of Word Representations in Vector Space
- Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean
- 2013
- https://arxiv.org/abs/1301.3781

### SGNS
- Revisiting Skip-Gram Negative Sampling Model with Rectification
- Cun Mu, Guang Yang, Zheng Yan
- 2018
- https://arxiv.org/abs/1804.00306

### GLOVE
- Global Vectors for Node Representations
- Robin Brochier, Adrien Guille, Julien Velcin
- 2019
- https://arxiv.org/abs/1902.11004

## ELMO/etc.

### BiRNN
- Bidirectional recurrent neural networks
- M. Schuster, K.K. Paliwal
- 1997
- https://ieeexplore.ieee.org/document/650093

### FastText
- Enriching Word Vectors with Subword Information
- Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov
- 2016
- https://arxiv.org/abs/1607.04606

### ELMo
- Deep contextualized word representations
- Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
- 2018
- https://arxiv.org/abs/1802.05365

## seq2seq
- Sequence to Sequence Learning with Neural Networks
- Ilya Sutskever, Oriol Vinyals, Quoc V. Le
- 2014
- https://arxiv.org/abs/1409.3215

## attention
- Neural Machine Translation by Jointly Learning to Align and Translate
- Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
- 2014
- https://arxiv.org/abs/1409.0473

## Transformer
- Attention Is All You Need
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
- 2017
- https://arxiv.org/abs/1706.03762

## GPT
- Improving Language Understanding by Generative Pre-Training
- Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
- 2018
- [paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

## BERT
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
- 2018
- https://arxiv.org/abs/1810.04805